#### LLAMA3 com RAG

#### comando 1: baixar e instalar o llama, mais informações sobre o modelo podem ser encontradas em: https://ollama.com/library/llama3
#### curl -fsSL https://ollama.com/install.sh | sh

#### comando 2: rodar o servidor
#### ollama serve

#### digite esses comandos, individualmente, dentro do segundo console que está rodando

#### comando 1: instalar o modelo llama3
#### ollama pull llama3

#### comando 2: instalar o modelo nomic-embed-text para embeddings
#### ollama pull nomic-embed-text

#### documentação docker: https://hub.docker.com/r/ollama/ollama